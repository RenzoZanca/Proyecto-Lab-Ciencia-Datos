# Documentaci√≥n del Pipeline Productivo SodAI Drinks

## Descripci√≥n General

Este pipeline de **Apache Airflow** implementa un sistema completo de machine learning para predecir compras de productos por parte de clientes. El sistema est√° dise√±ado para manejar datos incrementales y reentrenar autom√°ticamente cuando se detecta drift.

El pipeline implementa un enfoque robusto que evita data leakage mediante el c√°lculo de estad√≠sticas exclusivamente con datos de entrenamiento, garantizando la reproducibilidad y consistencia entre el entrenamiento y las predicciones en producci√≥n.

---

## Arquitectura del DAG

### **Diagrama de Flujo**

```mermaid

    A[start] --> B[check_data]
    
    %% Flujo principal si hay datos
    B --> C[get_data]
    C --> D[process_data]
    D --> E[holdout]
    E --> F[feature_engineering]
    F --> G[detect_drift]
    G --> H[decide_retraining]
    
    %% Flujo de reentrenamiento
    H --> I[optimize_hyperparameters]
    I --> J[train_model]
    J --> K[evaluate_model]
    K --> L[export_model]
    L --> M[model_ready]
    
    %% Flujo de copia de modelo
    H --> N[copy_previous_model]
    N --> M[model_ready]
    
    %% Flujo de predicciones
    M --> O[run_prediction]
    O --> P[get_products]
    P --> Q[end]
    
    %% Flujo de skip
    B --> R[skip_processing]
    R --> Q[end]
    
    style A fill:#e1f5fe
    style Q fill:#e8f5e8
    style B fill:#fff8e1
    style G fill:#fff3e0
    style H fill:#f3e5f5
    style M fill:#e8f5e8
    style R fill:#ffebee
```

---

## Descripci√≥n Detallada de Tareas

### **1. check_data**
**Funci√≥n**: `check_data_exists()`  
**Tipo**: BranchPythonOperator  
**Prop√≥sito**: Verifica la existencia de datos para la fecha de ejecuci√≥n  

**L√≥gica de Verificaci√≥n**:
- **Verifica existencia** del directorio `{execution_date}/data/`
- **Valida archivos requeridos**: `transacciones.parquet`, `clientes.parquet`, `productos.parquet`
- **Decisi√≥n inteligente**:
  - Si datos completos ‚Üí `return 'get_data'` (contin√∫a pipeline)
  - Si datos faltantes ‚Üí `return 'skip_processing'` (skip limpio)

**Implementaci√≥n T√©cnica**:
```python
def check_data_exists(**kwargs):
    execution_date = kwargs['ds']
    data_path = os.path.join(execution_date, "data")
    
    if os.path.exists(data_path):
        files_needed = ['transacciones.parquet', 'clientes.parquet', 'productos.parquet']
        if all(os.path.exists(os.path.join(data_path, f)) for f in files_needed):
            return 'get_data'
    return 'skip_processing'
```

**Importancia para Producci√≥n**:
- **Manejo robusto** de datos incrementales
- **Evita fallos** por datos faltantes
- **Optimiza recursos** saltando ejecuciones innecesarias

### **2. skip_processing**
**Tipo**: EmptyOperator  
**Prop√≥sito**: Terminaci√≥n limpia cuando no hay datos disponibles  

**Comportamiento**:
- Se ejecuta cuando `check_data` determina que no hay datos
- No genera errores, solo registra la condici√≥n
- Termina la ejecuci√≥n directamente en `end`

### **3. get_data**
**Funci√≥n**: `get_data()`  
**Prop√≥sito**: Obtiene los datos de la fuente  
**Entradas**: Archivos parquet en `{execution_date}/data/`
- `transacciones.parquet`
- `clientes.parquet` 
- `productos.parquet`

**Implementaci√≥n T√©cnica**:
- Utiliza el contexto de Airflow (`kwargs['ds']`) para determinar la fecha de ejecuci√≥n
- Crea directorios autom√°ticamente si no existen
- Implementa manejo de errores con logging descriptivo
- Valida la existencia de archivos antes de proceder

**Salidas**: Dictionary con DataFrames cargados  
**Consideraciones**: En producci√≥n, los datos aparecen en el directorio

### **4. process_data**
**Funci√≥n**: `process_data()`  
**Prop√≥sito**: Limpia y transforma los datos  

**Procesamiento Detallado**:

#### **Procesamiento de Clientes**:
- **Clustering Geogr√°fico**: Implementa DBSCAN con `eps=0.01` y `min_samples=10` para crear zonas geogr√°ficas personalizadas
- **Limpieza**: Eliminaci√≥n de duplicados y valores nulos
- **Validaci√≥n**: Verificaci√≥n de coordenadas v√°lidas

#### **Procesamiento de Productos**:
- **Normalizaci√≥n**: Estandarizaci√≥n de categor√≠as y subcategor√≠as
- **Validaci√≥n**: Verificaci√≥n de consistencia en marcas y segmentos

#### **Procesamiento de Transacciones**:
- **Filtrado**: Eliminaci√≥n de transacciones con items <= 0
- **C√°lculo Temporal**: Conversi√≥n a semanas ISO con manejo especial para finales de a√±o
- **Feature Temporal**: C√°lculo de `items_last_week` usando `.shift(1)` por grupo cliente-producto
- **Rolling Features**: C√°lculo de `items_roll4_mean` con ventana de 4 semanas

#### **Creaci√≥n de Combinaciones**:
- Producto cartesiano de todos los clientes, productos y semanas
- Enriquecimiento con datos de transacciones reales
- Creaci√≥n de etiquetas binarias (compra/no compra)

**Salidas**: `{execution_date}/data_processed/df_processed.parquet`

### **5. holdout**
**Funci√≥n**: `holdout()`  
**Prop√≥sito**: Divisi√≥n temporal de datos  

**Metodolog√≠a T√©cnica**:
- **Divisi√≥n Temporal**: Utiliza percentiles por semana para garantizar orden cronol√≥gico
  - 80% para entrenamiento (semanas m√°s tempranas)
  - 10% para validaci√≥n 
  - 10% para test (semanas m√°s recientes)

**Undersampling Estratificado**:
- **Funci√≥n**: `temporal_undersample()` con ratio 4:1
- **Estrategia**: Mantiene TODOS los ejemplos positivos, reduce negativos por semana
- **Aleatorizaci√≥n**: Usa `random_state=42` para reproducibilidad
- **Impacto**: Reduce dataset de entrenamiento de ~80% a ~35% del total

**Validaci√≥n de Balances**:
- Logging detallado de distribuciones de clases
- C√°lculo de m√©tricas de reducci√≥n por undersampling
- Verificaci√≥n de proporciones finales

**Sin Data Leakage**: Respeta orden cronol√≥gico estricto

### **6. feature_engineering**
**Funci√≥n**: `feature_engineering()`  
**Prop√≥sito**: Crear features y pipelines de preprocessing  

**Estad√≠sticas Calculadas (SOLO con datos de entrenamiento)**:

#### **Estad√≠sticas de Clientes**:
```python
client_trans = train_df.groupby("customer_id")["purchase_date"].count()
# -> total_transactions por cliente
```

#### **Estad√≠sticas Semanales**:
```python
weekly_agg = train_df.groupby(["customer_id","week"]).agg({
    "items": "sum",           # -> avg_items_per_week
    "product_id": "count"     # -> avg_products_per_week
}).groupby("customer_id").mean()
```

#### **Estad√≠sticas de Productos**:
```python
product_buyback = train_df.groupby("product_id")["purchase_date"].apply(
    lambda x: x.diff().mean().days  # -> avg_time_between_sales
)
```

**Pipeline de Preprocessing**:

#### **Features Num√©ricas**:
- `Y`, `X` (coordenadas geogr√°ficas)
- `num_deliver_per_week` (frecuencia de entrega)
- `items_last_week`, `items_roll4_mean` (features temporales)
- `size` (tama√±o del producto)
- `total_transactions`, `avg_products_per_week`, `avg_items_per_week` (estad√≠sticas calculadas)
- `avg_time_between_sales` (estad√≠stica de producto)

**Procesamiento**: SimpleImputer(median) + MinMaxScaler

#### **Features Categ√≥ricas**:
- `custom_zone` (cluster geogr√°fico)
- `customer_type`, `brand`, `category`, `sub_category`, `segment`, `package`
- `week`, `month`, `day` (features temporales extra√≠das)

**Procesamiento**: SimpleImputer(most_frequent) + OneHotEncoder

**Pipeline Final**:
```python
Pipeline([
    ("merge_trans_count", StaticFeatureMerger(client_trans, on="customer_id")),
    ("merge_weekly", StaticFeatureMerger(weekly_agg, on="customer_id")),
    ("merge_buyback", StaticFeatureMerger(product_buyback, on="product_id")),
    ("date_feats", FunctionTransformer(extract_date_features)),
    ("preprocessing", ColumnTransformer([...]))
])
```

**Cr√≠tico**: Usa **SOLO datos de entrenamiento** para estad√≠sticas

### **7. detect_drift** (BONUS)
**Funci√≥n**: `detect_drift()`  
**Prop√≥sito**: Sistema inteligente de detecci√≥n de drift con manejo de casos borde  

**L√≥gica de Referencias Inteligente**:

1. **B√∫squeda de Modelo Previo** (incremental):
   - Busca modelos de semanas anteriores (7, 14, 21... d√≠as hacia atr√°s)
   - Si encuentra modelo previo ‚Üí usa sus datos como referencia
   - Permite comparaci√≥n evolutiva entre ejecutciones

2. **Fallback a Baseline Hist√≥rico** (primera ejecuci√≥n):
   - Si no hay modelo previo ‚Üí usa datos de entrenamiento hist√≥ricos
   - Garantiza funcionamiento en primera ejecuci√≥n

**Metodolog√≠a T√©cnica Avanzada**:

#### **Features Num√©ricas**:
- **Test**: Kolmogorov-Smirnov de dos muestras
- **M√©tricas adicionales**: 
  - Diferencia normalizada de medias
  - Ratio de varianzas
  - P-value de significancia
- **Threshold**: p-value < 0.05

#### **Features Categ√≥ricas**:
- **Test**: Chi-cuadrado para distribuciones
- **Comparaci√≥n**: Distribuciones de frecuencias normalizadas
- **Manejo robusto**: Solo categor√≠as comunes entre datasets

#### **Sistema de Scoring**:
```python
# Drift score acumulativo
drift_score = 0.0
for feature in numeric_features:
    if p_value < 0.05:
        drift_score += 1.0

for feature in categorical_features:
    if p_value < 0.05:
        drift_score += 0.5  # Menor peso categ√≥rico

# Decisi√≥n final
drift_detected = drift_score >= 2.0  # Threshold configurable
```

**Features Monitoreadas**: 
- **Num√©ricas**: `Y`, `X`, `num_deliver_per_week`, `size`, `items_last_week`, `items_roll4_mean`
- **Categ√≥ricas**: `brand`, `category` (si disponibles)

**Manejo de Errores Robusto**:
- **Serializaci√≥n JSON segura**: Conversi√≥n expl√≠cita de tipos
- **Fallback autom√°tico**: Si falla an√°lisis detallado, guarda versi√≥n simplificada
- **Logging comprehensivo**: Incluye fuente de referencia y m√©tricas

**Salidas**: 
- `drift_results.json` (an√°lisis completo con metadata)
- `drift_detected.txt` (flag booleano para branching autom√°tico)

### **8. decide_retraining**
**Funci√≥n**: `decide_retraining()`  
**Tipo**: BranchPythonOperator  
**Prop√≥sito**: L√≥gica inteligente de decisi√≥n de reentrenamiento  

**Casos Manejados**:

#### **Caso 1: Primera Ejecuci√≥n**
```python
if not has_previous_model:
    print("üÜï PRIMERA EJECUCI√ìN - Entrenando modelo inicial")
    return 'optimize_hyperparameters'
```
- **B√∫squeda exhaustiva**: Verifica 52 semanas anteriores
- **Decisi√≥n autom√°tica**: Siempre entrena en primera ejecuci√≥n
- **Logging detallado**: Incluye paths verificados

#### **Caso 2: Ejecuci√≥n Incremental**
```python
drift_detected = read_drift_flag()
if drift_detected:
    return 'optimize_hyperparameters'  # Reentrenamiento
else:
    return 'copy_previous_model'       # Reutilizaci√≥n
```

**Implementaci√≥n T√©cnica**:
- **B√∫squeda de modelos**: Algoritmo de b√∫squeda temporal optimizado
- **Lectura de drift**: Parsing robusto de archivo de decisi√≥n
- **Manejo de errores**: Fallback a reentrenamiento por seguridad
- **Logging exhaustivo**: Traceback completo para debugging

**Branching Logic**:
- **optimize_hyperparameters** ‚Üí Flujo de reentrenamiento completo
- **copy_previous_model** ‚Üí Flujo de reutilizaci√≥n eficiente

### **9. copy_previous_model**
**Funci√≥n**: `copy_previous_model()`  
**Prop√≥sito**: Reutilizaci√≥n inteligente de modelos previos cuando no hay drift  

**Proceso de Copia**:

1. **B√∫squeda de Modelo M√°s Reciente**:
   - Itera semanas anteriores hasta encontrar `model_export/`
   - Valida integridad de todos los artefactos
   - Prioriza modelo m√°s reciente disponible

2. **Copia Completa de Artefactos**:
   ```python
   # Copia todos los componentes del modelo
   shutil.copy2("model.bin")              # Modelo entrenado
   shutil.copy2("features_pipeline.pkl")  # Pipeline de preprocessing
   shutil.copy2("threshold.txt")          # Umbral optimizado
   shutil.copy2("metrics.json")           # M√©tricas de evaluaci√≥n
   shutil.copytree("feature_stats/")      # Estad√≠sticas de entrenamiento
   ```

3. **Metadatos de Trazabilidad**:
   ```python
   copy_metadata = {
       'copied_from': prev_date_str,
       'copy_date': execution_date,
       'reason': 'no_drift_detected'
   }
   ```

**Ventajas de Eficiencia**:
- **Tiempo**: Evita reentrenamiento innecesario (minutos vs horas)
- **Recursos**: Ahorra CPU/memoria para optimizaci√≥n de hiperpar√°metros
- **Consistencia**: Mantiene modelo estable cuando datos son consistentes

**Manejo de Errores**:
- **Fallback autom√°tico**: Si falla copia ‚Üí trigger reentrenamiento
- **Validaci√≥n**: Verifica integridad antes de confirmar copia

### **10. optimize_hyperparameters**
**Funci√≥n**: `optimize_hyperparameters()`  
**Prop√≥sito**: Optimizaci√≥n con Optuna  

**Hiperpar√°metros Optimizados**:
- `learning_rate`: 0.10-0.25 (tasa de aprendizaje)
- `max_depth`: 8-16 (profundidad m√°xima del √°rbol)
- `subsample`: 0.65-1.0 (submuestreo de observaciones)
- `colsample_bytree`: 0.6-1.0 (submuestreo de features)
- `gamma`: 0.0-2.0 (regularizaci√≥n de complejidad)
- `min_child_weight`: 1-10 (peso m√≠nimo por hoja)
- `reg_alpha`: 0.1-5.0 (regularizaci√≥n L1)
- `reg_lambda`: 0.1-5.0 (regularizaci√≥n L2)

**Configuraci√≥n de Optuna**:
- **Sampler**: TPESampler con semilla fija (42)
- **Pruner**: MedianPruner para terminaci√≥n temprana
- **Timeout**: 300 segundos m√°ximo
- **M√©trica**: F1-Score en validaci√≥n

**B√∫squeda de Threshold**:
- **Rango**: Grid search entre 0.1 y 0.9 (81 puntos)
- **M√©trica**: F1-Score √≥ptimo en conjunto de validaci√≥n
- **Estrategia**: Maximizar F1 para manejar desbalance de clases

**Salida**: Archivo JSON con hiperpar√°metros √≥ptimos y threshold

### **11. train_model**
**Funci√≥n**: `train_model()`  
**Prop√≥sito**: Entrenamiento del modelo final  

**Proceso de Entrenamiento**:
1. **Carga de Par√°metros**: Lee hiperpar√°metros optimizados desde JSON
2. **Construcci√≥n de DMatrix**: Convierte datos a formato XGBoost optimizado
3. **Entrenamiento**: 
   - Utiliza par√°metros optimizados de Optuna
   - Early stopping con 30 rondas
   - Verbose deshabilitado para producci√≥n
4. **Evaluaci√≥n**: Genera predicciones en conjunto de test
5. **Aplicaci√≥n de Threshold**: Usa threshold optimizado para clasificaci√≥n binaria

**Modelo**: XGBoost con par√°metros optimizados  

**Salidas**: 
- `xgb_model.bin` (modelo entrenado serializado)
- `threshold.txt` (umbral √≥ptimo)
- `classification_report.txt` (reporte detallado de clasificaci√≥n)

### **12. evaluate_model**
**Funci√≥n**: `evaluate_model()`  
**Prop√≥sito**: Evaluaci√≥n comprehensiva del modelo  

**M√©tricas Calculadas**:
- **Accuracy**: Proporci√≥n de predicciones correctas
- **Precision**: Precisi√≥n en clase positiva (evita falsos positivos)
- **Recall**: Sensibilidad en clase positiva (detecta verdaderos positivos)
- **F1-Score**: Media arm√≥nica entre precision y recall
- **ROC-AUC**: √Årea bajo la curva ROC (discriminaci√≥n)

**Implementaci√≥n**:
```python
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

metrics = {
    'accuracy': accuracy_score(y_test, preds_test),
    'precision': precision_score(y_test, preds_test),
    'recall': recall_score(y_test, preds_test),
    'f1_score': f1_score(y_test, preds_test),
    'roc_auc': roc_auc_score(y_test, probs_test)
}
```

**Salidas**: `metrics.json` con todas las m√©tricas estructuradas

### **13. export_model**
**Funci√≥n**: `export_model()`  
**Prop√≥sito**: Exporta modelo y artefactos para producci√≥n  

**Artefactos Exportados**:
- **Modelo**: `model.bin` (XGBoost serializado)
- **Pipeline**: `features_pipeline.pkl` (preprocesamiento completo)
- **Threshold**: `threshold.txt` (umbral optimizado)
- **M√©tricas**: `metrics.json` (evaluaci√≥n del modelo)
- **Estad√≠sticas**: Directorio `feature_stats/` con:
  - `client_stats.parquet` (estad√≠sticas por cliente)
  - `weekly_stats.parquet` (promedios semanales)
  - `product_stats.parquet` (estad√≠sticas por producto)

**Importancia de las Estad√≠sticas**:
Estas estad√≠sticas son cr√≠ticas para mantener consistencia entre entrenamiento y predicci√≥n, evitando data leakage al usar siempre las mismas estad√≠sticas calculadas con datos de entrenamiento.

### **14. model_ready**
**Tipo**: EmptyOperator  
**Prop√≥sito**: Punto de convergencia de flujos de modelo  

**Funcionalidad**:
- **Uni√≥n de flujos**: Convergencia de `export_model` y `copy_previous_model`
- **Trigger rule**: `none_failed_min_one_success` - contin√∫a si cualquier flujo anterior tuvo √©xito
- **Sincronizaci√≥n**: Garantiza que modelo est√© listo antes de predicciones

**Dise√±o Arquitect√≥nico**:
```
optimize_hyperparameters ‚Üí train_model ‚Üí evaluate_model ‚Üí export_model ‚Üò
                                                                         model_ready ‚Üí run_prediction
decide_retraining ‚Üí copy_previous_model ‚Üó
```

**Beneficios**:
- **Flexibilidad**: Maneja ambos flujos (reentrenamiento/copia) de manera uniforme
- **Robustez**: Permite fallo de un flujo mientras otro tenga √©xito
- **Claridad**: Punto claro de transici√≥n entre l√≥gica de modelo y predicciones

### **15. run_prediction**
**Funci√≥n**: `run_prediction()`  
**Prop√≥sito**: Genera predicciones para la pr√≥xima semana  

**Metodolog√≠a Detallada**:

#### **Carga de Artefactos**:
- Modelo XGBoost entrenado
- Pipeline de preprocessing completo
- Threshold optimizado
- Estad√≠sticas de entrenamiento (cr√≠tico para consistencia)

#### **Funci√≥n `create_robust_prediction_dataset`**:

**Prop√≥sito**: Crear dataset de predicci√≥n robusto que maneja casos edge

**Implementaci√≥n T√©cnica**:

1. **Filtrado Temporal**:
```python
historical_data = df_processed[df_processed['week'] <= max_week]
```

2. **Generaci√≥n de Combinaciones**:
```python
combinations = list(itertools.product(unique_customers, unique_products))
prediction_df = pd.DataFrame(combinations, columns=['customer_id', 'product_id'])
```

3. **C√°lculo de Features Temporales**:
   - **items_last_week**: √öltimo valor registrado por par cliente-producto
   - **items_roll4_mean**: Media m√≥vil de 4 semanas usando hist√≥rico disponible
   - **Vectorizaci√≥n**: Operaciones optimizadas con pandas groupby

4. **Merge de Informaci√≥n Est√°tica**:
   - Informaci√≥n de clientes (demograf√≠a, ubicaci√≥n)
   - Informaci√≥n de productos (categor√≠as, caracter√≠sticas)

5. **Manejo de Casos Edge**:
```python
default_client = {
    'customer_type': 'Regular',
    'Y': 0.0, 'X': 0.0,
    'num_deliver_per_week': 1,
    'custom_zone': 0
}

default_product = {
    'brand': 'Generic',
    'category': 'Bebidas',
    'sub_category': 'Gaseosas',
    'segment': 'Regular',
    'package': 'Botella',
    'size': 500.0
}
```

**Robustez del Sistema**:
- **Clientes Nuevos**: Asigna valores por defecto autom√°ticamente
- **Productos Nuevos**: Usa caracter√≠sticas gen√©ricas
- **Combinaciones Inexistentes**: Inicializa con valores base (items_last_week=0)
- **Features Faltantes**: Pipeline de imputaci√≥n maneja autom√°ticamente

#### **Proceso de Predicci√≥n**:
1. **Transformaci√≥n**: Aplica pipeline de preprocessing id√©ntico al entrenamiento
2. **Predicci√≥n**: Genera probabilidades con XGBoost
3. **Clasificaci√≥n**: Aplica threshold optimizado
4. **Almacenamiento**: Guarda predicciones completas en parquet

### **16. get_products**
**Funci√≥n**: `get_products()`  
**Prop√≥sito**: Genera CSV final con predicciones positivas  

**Proceso de Filtrado**:
1. **Carga**: Lee predicciones completas desde parquet
2. **Filtrado**: Selecciona solo predicciones positivas (prediction == 1)
3. **Selecci√≥n de Columnas**: Mantiene solo informaci√≥n esencial
4. **Ordenamiento**: Ordena por probabilidad descendente
5. **Exportaci√≥n**: Genera CSV final para consumo

**Salida**: `recommended_products.csv`  
**Contenido**: Solo pares cliente-producto con predicci√≥n positiva, ordenados por probabilidad

**Estad√≠sticas Generadas**:
- Total de recomendaciones
- Clientes √∫nicos afectados
- Productos √∫nicos recomendados
- Estad√≠sticas de probabilidad (promedio, m√°ximo, m√≠nimo)

---

## Configuraci√≥n y Ejecuci√≥n

### **Configuraci√≥n del DAG**
```python
default_args = {
    'start_date': datetime(2024, 10, 1),
}

dag = DAG(
    dag_id='sodAI',
    schedule='@weekly',          # Ejecuci√≥n semanal autom√°tica
    catchup=True,                # Comportamiento incremental habilitado
    max_active_runs=1,           # Evita ejecuciones paralelas
    tags=['sodAI']
)
```

### **Par√°metros de Configuraci√≥n Cr√≠ticos**:

#### **Ejecuci√≥n Temporal**:
- **schedule='@weekly'**: Comportamiento incremental autom√°tico como requiere el enunciado
- **catchup=True**: Esencial para procesar datos hist√≥ricos y futuros incrementales
- **start_date**: Define punto de inicio para backfill autom√°tico

#### **Control de Recursos**:
- **max_active_runs=1**: Previene sobrecarga de recursos por ejecuciones paralelas
- **Evita**: Los problemas de SIGKILL observados anteriormente

#### **Branching y Flujos**:
- **BranchPythonOperator**: Para decisiones inteligentes (check_data, decide_retraining)
- **EmptyOperator con trigger_rule**: Para convergencia de flujos (model_ready)
- **Dependencias condicionales**: Optimizaci√≥n autom√°tica de rutas de ejecuci√≥n



### **Monitoreo**
- **Logs de Airflow**: Informaci√≥n detallada de cada tarea con timestamps
- **Drift Detection**: Alertas autom√°ticas en logs con p-values
- **M√©tricas del Modelo**: JSON estructurado para an√°lisis posterior
- **Performance Metrics**: Tiempo de ejecuci√≥n por tarea

---

## Representaci√≥n Visual del DAG

![DAG en Airflow](./dag_screenshot.png)

*Nota: Screenshot del DAG ejecut√°ndose en la interfaz de Airflow*

### **Estados de Tareas**
- **Success**: Tarea completada exitosamente
- **Failed**: Error en ejecuci√≥n con logs detallados
- **Running**: Ejecut√°ndose actualmente con progreso
- **Pending**: En cola de ejecuci√≥n esperando recursos

### **Dependencias y Flujos Condicionales**:

#### **Arquitectura de Branching**:
El DAG implementa l√≥gica condicional avanzada con m√∫ltiples puntos de decisi√≥n:

1. **Primer Branch - Verificaci√≥n de Datos**:
   ```
   start ‚Üí check_data ‚Üí [get_data | skip_processing]
   ```

2. **Segundo Branch - Estrategia de Modelo**:
   ```
   detect_drift ‚Üí decide_retraining ‚Üí [optimize_hyperparameters | copy_previous_model]
   ```

3. **Convergencia Inteligente**:
   ```
   [export_model | copy_previous_model] ‚Üí model_ready ‚Üí run_prediction
   ```

#### **Trigger Rules Especializadas**:
- **model_ready**: `none_failed_min_one_success` permite convergencia robusta
- **Flujos paralelos**: Manejo independiente de reentrenamiento vs copia
- **Fallback autom√°tico**: Errores en un flujo no bloquean el pipeline completo

#### **Beneficios Arquitect√≥nicos**:
- **Eficiencia**: Evita procesamiento innecesario
- **Robustez**: M√∫ltiples rutas de √©xito 
- **Escalabilidad**: Optimizaci√≥n autom√°tica de recursos
- **Mantenibilidad**: L√≥gica clara y modular

---


---

## Pr√≥ximas Mejoras

### **Funcionalidades Futuras**
- [ ] **MLflow Integration**: Tracking completo de experimentos


---


